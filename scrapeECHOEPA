#!/usr/bin/env python3 
#Imports
import csv
import csvkit
import os 
import sys
import subprocess
from datetime import datetime
import glob
import db_connect
import EXP_PGM as pgm #script for creating views based on raw ECHOEPA data
import viewsToTable as mview #script which creates materialized views of view 
from sqlalchemy import create_engine

#Set paths
os.chdir(os.path.dirname(sys.argv[0]))

#Connect to DB
engine,whichDb= db_connect.connect()

#
def truncate(table):
	print(table)
	try:
		engine.execute('truncate "%s"' %(table))
	except Exception as e:
		print (sys.exc_info()[0])
		print (e.__class__.__name__)
		sys.exit("Exit") 

#used to compare number of rows in download csv to sql count(*)
def checkSanity(table):
	file=open("CSV/%s.csv" %table)
	reader=csv.reader(file)
	csv_count=len(list(reader))-1
	sql_count=list(engine.execute('select count(*) from "%s"' %table).fetchone())[0]
	engine.execute("update \"Last-Modified\" set \"csv_count\"=%s,\"sql_count\"=%s where \"name\"='%s'" %(csv_count,sql_count,table ));
	print("update \"Last-Modified\" set \"csv_count\"=%s,\"sql_count\"=%s where \"name\"='%s'" %(csv_count,sql_count,table ));

#commented out to run maually for now
#os.system("./wgetGSheet")

#Open list of files to scrape and add
with open('files.csv', newline='') as csvfile:
	reader = csv.DictReader(csvfile)
	uniqueZips={}
	tables=[]
	spatialZips={}
	spatialTables=[]

	#For each data source, check how to process it
	for row in reader:
		if row['TYPE'] == "Attribute":
			tables.append(row['CSV FILE'])
			if row['Location'] not in uniqueZips.keys():
				uniqueZips[row['Location']] = row['URL']
		elif row['TYPE'] == "Spatial":
			print("spatial")
			spatialTables.append(row['CSV FILE'])
			spatialZips[row['Location']] = row['URL']

	#For each spatial file, get and unzip
	for location, url in spatialZips.items():
		print(location, url)
		if not os.path.isfile('zips/%s' %(location)):
			print("Getting %s" %(url))
			subprocess.check_output("./wgetEPA %s" %(url), shell=True) #Get zip from link. Note: not yet checking "modified"
			os.system("./unzipEPA %s" %(location)) #Unzip. Currently unzipping to CSV folder
		else:		
			print("Zip file %s  already exists, unzipping..." %(location))
			os.system("./unzipEPA %s" %(location))
		
	#For each spatial table, insert into DB
	for table in spatialTables:
		if os.path.isfile('CSV/%s.shp' %(table)):
			cmd = "shp2pgsql -d -I -s 4326 {0} public.{1}| psql -h database -d slim_echoepa -U echoepa".format('CSV/'+table, table) #Inserts data. Must already be in schema.
			print(cmd) #debugging
			subprocess.call(cmd, shell=True)
		else:
			print("no spatial path")

	#For each unique zip of attribute data, get and unzip it
	for location, url in uniqueZips.items():
		print(location, url)
		#break
		if not os.path.isfile('zips/%s' %(location)):
			modified=""
			while(modified==""):
				print("Getting %s" %(url))
				modified = subprocess.check_output("./wgetEPA %s" %(url), shell=True).decode("utf-8").strip().splitlines()[0]
			results = engine.execute("update \"Last-Modified\" set \"modified\"='%s' where \"zip\" = '%s'" %(modified,location))
			if results.rowcount == 0:
				results = engine.execute("insert into \"Last-Modified\" (\"modified\", \"zip\") values ('%s', '%s')" %(modified,location))
			We may not always have zip files that need unzipping, so we just move the downloaded CSV to the CSV folder
			if ".csv" in url:
				os.rename("zips/"+location, "CSV/"+location) #move file
			elif ".zip" in url:
				os.system("./unzipEPA %s" %(location)) #Unzip
		else:		
			print("Zip file %s  already exists, unzipping or moving..." %(location))
			#break
			#We may not always have zip files that need unzipping, so we just move the downloaded CSV to the CSV folder
			if ".csv" in url:
				os.rename("zips/"+location, "CSV/"+location) #move file
			elif ".zip" in url:
				os.system("./unzipEPA %s" %(location)) #comment out for testing
	
	#For each table, insert unzipped CSV into DB or create new table in schema and insert
	for table in tables:
		print(table)
		#test if table already in db...
		try:
			test = engine.execute("select 'public.\"%s\"'::regclass" %(table))
		except:
			print("no table yet exists")
			t = subprocess.check_output("./createTable %s" %(table), shell=True).decode("utf-8").strip()
			#create the table
			engine.execute(t)
		#break
		os.system("./stripNulls %s" %(table)) #some csvs failed to import with nulls
		truncate(table)
		print ("truncated %s"%(table))
		os.system("./csvImport %s %s" %(table,whichDb)) # import the data
		checkSanity(table)
	
	#Create views based off of ECHOEPA tables
	pgm.build(engine)
	mview.build(engine) #materialize views
	engine.execute("analyze");

#Clear up files
#for csv in glob.glob("./CSV/*.csv"):
	#os.remove(csv)		
#for zip in glob.glob("./zips/*.zip"):
	#os.remove(zip)

#Switch DB
db_connect.change_db()
#Stony Brook Specific way to update remote currentDBIndex
#os.system("./toggleIndex")
